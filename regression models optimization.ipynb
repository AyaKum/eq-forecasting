{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, r2_score, explained_variance_score, median_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# Define the API endpoint URL\n",
    "api_url = \"https://earthquake.usgs.gov/fdsnws/event/1/query\"\n",
    "\n",
    "current_time = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "# Define the parameters\n",
    "params = {\n",
    "    'format': 'csv',\n",
    "    'starttime': '1960-01-01',\n",
    "    'endtime': current_time,\n",
    "    'latitude': 43.75,\n",
    "    'longitude': 77,\n",
    "    'maxradiuskm': 500,\n",
    "    'minmagnitude': 3,\n",
    "    'orderby': 'time'\n",
    "}\n",
    "\n",
    "# Make the API request\n",
    "response = requests.get(api_url, params=params)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Convert CSV data to Pandas DataFrame\n",
    "    data = pd.read_csv(StringIO(response.text))\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    data.to_csv('earthquake_dataset.csv', index=False)\n",
    "\n",
    "    # Print the first few rows of the DataFrame\n",
    "    print(data)\n",
    "else:\n",
    "    # Print an error message if the request was not successful\n",
    "    print(f\"Error: {response.status_code} - {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>time</th>\n",
       "      <th>depth</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>magnitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.0956</td>\n",
       "      <td>78.3679</td>\n",
       "      <td>2024-09-03 15:47:00.721000+00:00</td>\n",
       "      <td>13.353</td>\n",
       "      <td>2024</td>\n",
       "      <td>9</td>\n",
       "      <td>2024</td>\n",
       "      <td>15</td>\n",
       "      <td>4.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41.3620</td>\n",
       "      <td>78.7243</td>\n",
       "      <td>2024-08-21 21:14:37.981000+00:00</td>\n",
       "      <td>10.000</td>\n",
       "      <td>2024</td>\n",
       "      <td>8</td>\n",
       "      <td>2024</td>\n",
       "      <td>21</td>\n",
       "      <td>4.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39.9773</td>\n",
       "      <td>74.1667</td>\n",
       "      <td>2024-08-21 04:29:39.843000+00:00</td>\n",
       "      <td>10.000</td>\n",
       "      <td>2024</td>\n",
       "      <td>8</td>\n",
       "      <td>2024</td>\n",
       "      <td>4</td>\n",
       "      <td>4.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42.2223</td>\n",
       "      <td>72.0394</td>\n",
       "      <td>2024-08-10 05:26:39.615000+00:00</td>\n",
       "      <td>17.642</td>\n",
       "      <td>2024</td>\n",
       "      <td>8</td>\n",
       "      <td>2024</td>\n",
       "      <td>5</td>\n",
       "      <td>4.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.0627</td>\n",
       "      <td>77.2831</td>\n",
       "      <td>2024-08-06 01:32:31.635000+00:00</td>\n",
       "      <td>10.000</td>\n",
       "      <td>2024</td>\n",
       "      <td>8</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>3.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2048</th>\n",
       "      <td>39.8780</td>\n",
       "      <td>77.4250</td>\n",
       "      <td>1961-12-30 07:08:37.030000+00:00</td>\n",
       "      <td>35.000</td>\n",
       "      <td>1961</td>\n",
       "      <td>12</td>\n",
       "      <td>1961</td>\n",
       "      <td>7</td>\n",
       "      <td>5.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2049</th>\n",
       "      <td>39.7620</td>\n",
       "      <td>77.7120</td>\n",
       "      <td>1961-04-13 16:34:44.590000+00:00</td>\n",
       "      <td>35.000</td>\n",
       "      <td>1961</td>\n",
       "      <td>4</td>\n",
       "      <td>1961</td>\n",
       "      <td>16</td>\n",
       "      <td>7.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2050</th>\n",
       "      <td>39.8130</td>\n",
       "      <td>77.7200</td>\n",
       "      <td>1961-04-06 01:33:51.550000+00:00</td>\n",
       "      <td>25.000</td>\n",
       "      <td>1961</td>\n",
       "      <td>4</td>\n",
       "      <td>1961</td>\n",
       "      <td>1</td>\n",
       "      <td>5.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2051</th>\n",
       "      <td>39.8580</td>\n",
       "      <td>77.9100</td>\n",
       "      <td>1961-04-04 09:46:43.780000+00:00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>1961</td>\n",
       "      <td>4</td>\n",
       "      <td>1961</td>\n",
       "      <td>9</td>\n",
       "      <td>6.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2052</th>\n",
       "      <td>39.8570</td>\n",
       "      <td>77.8410</td>\n",
       "      <td>1961-04-01 15:18:28.370000+00:00</td>\n",
       "      <td>20.000</td>\n",
       "      <td>1961</td>\n",
       "      <td>4</td>\n",
       "      <td>1961</td>\n",
       "      <td>15</td>\n",
       "      <td>6.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2053 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      latitude  longitude                             time   depth  year  \\\n",
       "0      41.0956    78.3679 2024-09-03 15:47:00.721000+00:00  13.353  2024   \n",
       "1      41.3620    78.7243 2024-08-21 21:14:37.981000+00:00  10.000  2024   \n",
       "2      39.9773    74.1667 2024-08-21 04:29:39.843000+00:00  10.000  2024   \n",
       "3      42.2223    72.0394 2024-08-10 05:26:39.615000+00:00  17.642  2024   \n",
       "4      40.0627    77.2831 2024-08-06 01:32:31.635000+00:00  10.000  2024   \n",
       "...        ...        ...                              ...     ...   ...   \n",
       "2048   39.8780    77.4250 1961-12-30 07:08:37.030000+00:00  35.000  1961   \n",
       "2049   39.7620    77.7120 1961-04-13 16:34:44.590000+00:00  35.000  1961   \n",
       "2050   39.8130    77.7200 1961-04-06 01:33:51.550000+00:00  25.000  1961   \n",
       "2051   39.8580    77.9100 1961-04-04 09:46:43.780000+00:00  20.000  1961   \n",
       "2052   39.8570    77.8410 1961-04-01 15:18:28.370000+00:00  20.000  1961   \n",
       "\n",
       "      month   day  hour  magnitude  \n",
       "0         9  2024    15       4.30  \n",
       "1         8  2024    21       4.50  \n",
       "2         8  2024     4       4.30  \n",
       "3         8  2024     5       4.20  \n",
       "4         8  2024     1       3.90  \n",
       "...     ...   ...   ...        ...  \n",
       "2048     12  1961     7       5.82  \n",
       "2049      4  1961    16       7.04  \n",
       "2050      4  1961     1       5.79  \n",
       "2051      4  1961     9       6.34  \n",
       "2052      4  1961    15       6.77  \n",
       "\n",
       "[2053 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = {'latitude': data.latitude, 'longitude': data.longitude, 'time': pd.to_datetime(data['time']),\n",
    "     'depth': data.depth, 'year': pd.DatetimeIndex(data.time).year, 'month': pd.DatetimeIndex(data.time).month,'day': pd.DatetimeIndex(data.time).year,'hour': pd.DatetimeIndex(data.time).hour,'magnitude': data.mag}\n",
    "df = pd.DataFrame(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rolling_mean_magnitude'] = df['magnitude'].rolling(window=10, min_periods=1).mean()\n",
    "df['time_since_last_hour'] = df['time'].diff().dt.total_seconds().div(3600).abs()\n",
    "print(df.isna().sum())\n",
    "df = df.fillna(0)\n",
    "print(df.isna().sum())\n",
    "\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Defining significant earthquakes\n",
    "significant_threshold = 6.0\n",
    "significant_earthquakes = df[df['magnitude'] >= significant_threshold].copy()\n",
    "\n",
    "# Ensuring the time is in datetime format if not already\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "significant_earthquakes['time'] = pd.to_datetime(significant_earthquakes['time'])\n",
    "def find_closest_earthquake(row, significant_df):\n",
    "    # Calculate distances\n",
    "    distances = significant_df.apply(lambda x: geodesic((x['latitude'], x['longitude']), (row['latitude'], row['longitude'])).kilometers, axis=1)\n",
    "    min_distance_index = distances.idxmin()\n",
    "    closest_earthquake = significant_df.loc[min_distance_index]\n",
    "\n",
    "    # Calculate time difference in days\n",
    "    time_difference = abs((closest_earthquake['time'] - row['time']).total_seconds() / 86400)\n",
    "\n",
    "    return pd.Series([closest_earthquake['time'], min_distance_index, distances[min_distance_index], time_difference])\n",
    "\n",
    "# Apply the function to each row\n",
    "df[['closest_eq_time', 'closest_eq_index', 'distance_to_closest_eq', 'time_diff_to_closest_eq']] = df.apply(find_closest_earthquake, significant_df=significant_earthquakes, axis=1)\n",
    "print(df.isna().sum())\n",
    "\n",
    "\n",
    "df = df.set_index('time', inplace=False)\n",
    "df = df.drop(['hour', 'closest_eq_time', 'closest_eq_index'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('magnitude', axis=1)\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "#print(X)\n",
    "\n",
    "y = df['magnitude']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# train_data = df[df['year'] < 2020]\n",
    "# test_data = df[df['year'] >= 2020]\n",
    "\n",
    "# X_train = train_data.drop('magnitude', axis=1)\n",
    "# X_test = test_data.drop('magnitude', axis=1)\n",
    "\n",
    "# # Scaling the features using RobustScaler\n",
    "# scaler = RobustScaler()\n",
    "# X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "# X_test_scaled = pd.DataFrame(scaler.fit_transform(X_test), columns=X_test.columns)\n",
    "\n",
    "# y_train = train_data['magnitude']\n",
    "# y_test = test_data['magnitude']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T05:54:37.925531Z",
     "iopub.status.busy": "2024-09-09T05:54:37.925052Z",
     "iopub.status.idle": "2024-09-09T05:56:35.148145Z",
     "shell.execute_reply": "2024-09-09T05:56:35.146972Z",
     "shell.execute_reply.started": "2024-09-09T05:54:37.925496Z"
    }
   },
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "np.int = int\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "# Create KNeighborsRegressor model\n",
    "knn_model = KNeighborsRegressor()\n",
    "\n",
    "# Define parameter search space\n",
    "param_space = {\n",
    "    'n_neighbors': (5, 30),  # Adjust the upper limit accordingly\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': (1, 2),\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski']\n",
    "}\n",
    "# Perform Bayesian optimization\n",
    "bayes_search = BayesSearchCV(\n",
    "    knn_model,\n",
    "    param_space,\n",
    "    cv=5,\n",
    "    scoring=scorer, #'neg_mean_squared_error',\n",
    "    n_iter=50,  # Number of parameter settings that are sampled\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the Bayesian search to the training data\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = bayes_search.best_params_\n",
    "\n",
    "# Create a new KNeighborsRegressor model with the best hyperparameters\n",
    "best_knn_model = KNeighborsRegressor(n_neighbors=best_params['n_neighbors'], weights=best_params['weights'])\n",
    "\n",
    "# Fit the model to the training data\n",
    "best_knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_knn = best_knn_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mae_knn = mean_absolute_error(y_test, y_pred_knn)\n",
    "mse_knn = mean_squared_error(y_test, y_pred_knn)\n",
    "mape_knn = mean_absolute_percentage_error(y_test, y_pred_knn)\n",
    "r2_knn = r2_score(y_test, y_pred_knn)\n",
    "evs_knn = explained_variance_score(y_test, y_pred_knn)\n",
    "medae_knn = median_absolute_error(y_test, y_pred_knn)\n",
    "\n",
    "# Print evaluation metrics and best hyperparameters\n",
    "print(\"KNN Metrics:\", best_params)\n",
    "print(\"MAE:\", mae_knn)\n",
    "print(\"MSE:\", mse_knn)\n",
    "print(\"MAPE:\", mape_knn)\n",
    "print(\"R-squared:\", r2_knn)\n",
    "print(\"Explained Variance Score:\", evs_knn)\n",
    "print(\"Median Absolute Error:\", medae_knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T06:02:28.556013Z",
     "iopub.status.busy": "2024-09-09T06:02:28.555537Z",
     "iopub.status.idle": "2024-09-09T06:02:29.032742Z",
     "shell.execute_reply": "2024-09-09T06:02:29.031152Z",
     "shell.execute_reply.started": "2024-09-09T06:02:28.555980Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the search results to a DataFrame\n",
    "results_df = pd.DataFrame(bayes_search.cv_results_)\n",
    "\n",
    "# Pivot the DataFrame to plot a heatmap for two hyperparameters (e.g., n_neighbors and p)\n",
    "heatmap_data = results_df.pivot_table(values='mean_test_score', index='param_n_neighbors', columns='param_p')\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(heatmap_data, cmap='viridis', annot=True)\n",
    "plt.title('KNN Model Performance for n_neighbors and p')\n",
    "plt.xlabel('p (Minkowski distance)')\n",
    "plt.ylabel('n_neighbors')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Create a pipeline with XGBoost Regressor\n",
    "xgb_reg_model = XGBRegressor()\n",
    "\n",
    "# Define the parameter search space for XGBoost\n",
    "param_space_xgb = {\n",
    "    'max_depth': (3, 10),  # Maximum tree depth\n",
    "    'learning_rate': (0.01, 1.0, 'log-uniform'),  # Learning rate\n",
    "    'n_estimators': (50, 200),  # Number of trees\n",
    "    'gamma': (0.001, 1.0, 'log-uniform'),  # Regularization parameter\n",
    "}\n",
    "\n",
    "# Define the scoring metric (negative mean squared error)\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "# Perform Bayesian optimization for XGBoost\n",
    "bayes_search_xgb = BayesSearchCV(\n",
    "    xgb_reg_model,\n",
    "    param_space_xgb,\n",
    "    scoring=scorer,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    n_iter=50,  # Number of parameter settings that are sampled\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the Bayesian search to the data\n",
    "bayes_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params_xgb = bayes_search_xgb.best_params_\n",
    "\n",
    "# Train the XGBoost Regressor with the best parameters\n",
    "best_xgb_reg_model = XGBRegressor(**best_params_xgb)\n",
    "best_xgb_reg_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_xgb = best_xgb_reg_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "mape_xgb = mean_absolute_percentage_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "evs_xgb = explained_variance_score(y_test, y_pred_xgb)\n",
    "medae_xgb = median_absolute_error(y_test, y_pred_xgb)\n",
    "\n",
    "# Print or use the metrics as needed\n",
    "print(\"Best Parameters for XGBoost Regressor:\", best_params_xgb)\n",
    "print(\"XGBoost Regressor Metrics:\")\n",
    "print(\"MAE:\", mae_xgb)\n",
    "print(\"MSE:\", mse_xgb)\n",
    "print(\"MAPE:\", mape_xgb)\n",
    "print(\"R-squared:\", r2_xgb)\n",
    "print(\"Explained Variance Score:\", evs_xgb)\n",
    "print(\"Median Absolute Error:\", medae_xgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desicion tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T04:23:10.686591Z",
     "iopub.status.busy": "2024-02-23T04:23:10.686157Z",
     "iopub.status.idle": "2024-02-23T04:23:19.017906Z",
     "shell.execute_reply": "2024-02-23T04:23:19.016136Z",
     "shell.execute_reply.started": "2024-02-23T04:23:10.686558Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Define the parameter search space\n",
    "param_space = {\n",
    "    'max_depth': (1, 50),\n",
    "    'min_samples_split': (2, 30),\n",
    "    'min_samples_leaf': (1, 30),\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "# Create the decision tree regressor\n",
    "dt_model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Define the scoring metric (negative mean squared error)\n",
    "\n",
    "# Perform Bayesian optimization\n",
    "bayes_search = BayesSearchCV(\n",
    "    dt_model,\n",
    "    param_space,\n",
    "    scoring=scorer,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    n_iter=30,  # Number of parameter settings that are sampled\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the Bayesian search to the data\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params_dt = bayes_search.best_params_\n",
    "\n",
    "# Train the Decision Tree with the best parameters\n",
    "best_dt_model = DecisionTreeRegressor(\n",
    "    max_depth=best_params_dt['max_depth'],\n",
    "    min_samples_split=best_params_dt['min_samples_split'],\n",
    "    min_samples_leaf=best_params_dt['min_samples_leaf'],\n",
    "    random_state=42\n",
    ")\n",
    "best_dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_dt = best_dt_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mae_dt = mean_absolute_error(y_test, y_pred_dt)\n",
    "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
    "mape_dt = mean_absolute_percentage_error(y_test, y_pred_dt)\n",
    "r2_dt= r2_score(y_test, y_pred_dt)\n",
    "evs_dt = explained_variance_score(y_test, y_pred_dt)\n",
    "medae_dt = median_absolute_error(y_test, y_pred_dt)\n",
    "\n",
    "# Print or use the metrics as needed\n",
    "print(\"Best Parameters for Decision Tree:\", best_params_dt)\n",
    "print(\"Decision Tree Metrics:\")\n",
    "print(\"MAE:\", mae_dt)\n",
    "print(\"MSE:\", mse_dt)\n",
    "print(\"MAPE:\", mape_dt)\n",
    "print(\"R-squared:\", r2_dt)\n",
    "print(\"Explained Variance Score:\", evs_dt)\n",
    "print(\"Median Absolute Error:\", medae_dt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T04:23:19.020815Z",
     "iopub.status.busy": "2024-02-23T04:23:19.020309Z",
     "iopub.status.idle": "2024-02-23T04:26:39.319759Z",
     "shell.execute_reply": "2024-02-23T04:26:39.318277Z",
     "shell.execute_reply.started": "2024-02-23T04:23:19.020773Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, r2_score, explained_variance_score, median_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "\n",
    "# Define the parameter search space\n",
    "param_space = {\n",
    "    'n_estimators': (10, 200),\n",
    "    'max_depth': (1, 30),\n",
    "    'min_samples_split': (2, 100),\n",
    "    'min_samples_leaf': (2, 100)\n",
    "}\n",
    "\n",
    "# Create the Random Forest regressor\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Define the scoring metric (negative mean squared error)\n",
    "\n",
    "\n",
    "# Perform Bayesian optimization\n",
    "bayes_search = BayesSearchCV(\n",
    "    rf_model,\n",
    "    param_space,\n",
    "    scoring=scorer,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    n_iter=50,  # Number of parameter settings that are sampled\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the Bayesian search to the data\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params_rf = bayes_search.best_params_\n",
    "\n",
    "# Train the Random Forest with the best parameters\n",
    "best_rf_model = RandomForestRegressor(**best_params_rf, random_state=42)\n",
    "best_rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_rf = best_rf_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "mape_rf = mean_absolute_percentage_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "evs_rf = explained_variance_score(y_test, y_pred_rf)\n",
    "medae_rf = median_absolute_error(y_test, y_pred_rf)\n",
    "\n",
    "# Print or use the metrics as needed\n",
    "print(\"Best Parameters for Random Forest:\", best_params_rf)\n",
    "print(\"Random Forest Metrics:\")\n",
    "print(\"MAE:\", mae_rf)\n",
    "print(\"MSE:\", mse_rf)\n",
    "print(\"MAPE:\", mape_rf)\n",
    "print(\"R-squared:\", r2_rf)\n",
    "print(\"Explained Variance Score:\", evs_rf)\n",
    "print(\"Median Absolute Error:\", medae_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T04:28:09.426029Z",
     "iopub.status.busy": "2024-02-23T04:28:09.425594Z",
     "iopub.status.idle": "2024-02-23T04:28:10.752067Z",
     "shell.execute_reply": "2024-02-23T04:28:10.750831Z",
     "shell.execute_reply.started": "2024-02-23T04:28:09.425976Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Define the parameter search space\n",
    "param_space = {\n",
    "    'n_estimators': (10, 200),\n",
    "    'learning_rate': (0.01, 1.0, 'log-uniform'),\n",
    "    'max_depth': (1, 30), \n",
    "    'min_samples_split': (2, 10),\n",
    "    'min_samples_leaf': (1, 10)\n",
    "}\n",
    "\n",
    "# Create the Gradient Boosting regressor\n",
    "gb_model = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Define the scoring metric (negative mean squared error)\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "# Perform Bayesian optimization\n",
    "bayes_search = BayesSearchCV(\n",
    "    gb_model,\n",
    "    param_space,\n",
    "    scoring=scorer,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    n_iter=50,  # Number of parameter settings that are sampled\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the Bayesian search to the data\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params_gb = bayes_search.best_params_\n",
    "\n",
    "# Train the Gradient Boosting with the best parameters\n",
    "best_gb_model = GradientBoostingRegressor(**best_params_gb, random_state=42)\n",
    "best_gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_gb = best_gb_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mae_gb = mean_absolute_error(y_test, y_pred_gb)\n",
    "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
    "mape_gb = mean_absolute_percentage_error(y_test, y_pred_gb)\n",
    "r2_gb = r2_score(y_test, y_pred_gb)\n",
    "evs_gb = explained_variance_score(y_test, y_pred_gb)\n",
    "medae_gb = median_absolute_error(y_test, y_pred_gb)\n",
    "\n",
    "# Print or use the metrics as needed\n",
    "print(\"Best Parameters for Gradient Boosting:\", best_params_gb)\n",
    "print(\"Gradient Boosting Metrics:\")\n",
    "print(\"MAE:\", mae_gb)\n",
    "print(\"MSE:\", mse_gb)\n",
    "print(\"MAPE:\", mape_gb)\n",
    "print(\"R-squared:\", r2_gb)\n",
    "print(\"Explained Variance Score:\", evs_gb)\n",
    "print(\"Median Absolute Error:\", medae_gb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Define the parameter search space\n",
    "param_space = {\n",
    "    'C': (1e-3, 1e+3, 'log-uniform'),\n",
    "    'gamma': (1e-6, 1e+1, 'log-uniform'),\n",
    "    # 'epsilon': (1e-6, 1e+1, 'log-uniform')\n",
    "}\n",
    "\n",
    "# Create the SVR model\n",
    "svr_model = SVR()\n",
    "\n",
    "# Define the scoring metric (negative mean squared error)\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "# Perform Bayesian optimization\n",
    "bayes_search = BayesSearchCV(\n",
    "    svr_model,\n",
    "    param_space,\n",
    "    scoring=scorer,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    n_iter=50,  # Number of parameter settings that are sampled\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the Bayesian search to the data\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params_svr = bayes_search.best_params_\n",
    "\n",
    "# Train the SVR with the best parameters\n",
    "best_svr_model = SVR(**best_params_svr)\n",
    "best_svr_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_svr = best_svr_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mae_svr = mean_absolute_error(y_test, y_pred_svr)\n",
    "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
    "mape_svr = mean_absolute_percentage_error(y_test, y_pred_svr)\n",
    "r2_svr = r2_score(y_test, y_pred_svr)\n",
    "evs_svr = explained_variance_score(y_test, y_pred_svr)\n",
    "medae_svr = median_absolute_error(y_test, y_pred_svr)\n",
    "\n",
    "# Print or use the metrics as needed\n",
    "print(\"Best Parameters for SVR:\", best_params_svr)\n",
    "print(\"SVR Metrics:\")\n",
    "print(\"MAE:\", mae_svr)\n",
    "print(\"MSE:\", mse_svr)\n",
    "print(\"MAPE:\", mape_svr)\n",
    "print(\"R-squared:\", r2_svr)\n",
    "print(\"Explained Variance Score:\", evs_svr)\n",
    "print(\"Median Absolute Error:\", medae_svr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, r2_score, explained_variance_score, median_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter search space\n",
    "param_space = {\n",
    "    'poly__degree': (1, 5),\n",
    "    'poly__interaction_only': [True, False]\n",
    "}\n",
    "\n",
    "# Create a pipeline with PolynomialFeatures and LinearRegression\n",
    "poly_reg_model = Pipeline([\n",
    "    ('poly', PolynomialFeatures()),\n",
    "    ('reg', LinearRegression())\n",
    "])\n",
    "\n",
    "# Define the scoring metric (negative mean squared error)\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "# Perform Bayesian optimization\n",
    "bayes_search = BayesSearchCV(\n",
    "    poly_reg_model,\n",
    "    param_space,\n",
    "    scoring=scorer,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    n_iter=50,  # Number of parameter settings that are sampled\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the Bayesian search to the data\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params_poly_reg = bayes_search.best_params_\n",
    "\n",
    "# Train the Polynomial Regression with the best parameters\n",
    "best_poly_reg_model = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=best_params_poly_reg['poly__degree'], interaction_only=best_params_poly_reg['poly__interaction_only'])),\n",
    "    ('reg', LinearRegression())\n",
    "])\n",
    "best_poly_reg_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_poly_reg = best_poly_reg_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mae_poly_reg = mean_absolute_error(y_test, y_pred_poly_reg)\n",
    "mse_poly_reg = mean_squared_error(y_test, y_pred_poly_reg)\n",
    "mape_poly_reg = mean_absolute_percentage_error(y_test, y_pred_poly_reg)\n",
    "r2_poly_reg = r2_score(y_test, y_pred_poly_reg)\n",
    "evs_poly_reg = explained_variance_score(y_test, y_pred_poly_reg)\n",
    "medae_poly_reg = median_absolute_error(y_test, y_pred_poly_reg)\n",
    "\n",
    "# Print or use the metrics as needed\n",
    "print(\"Best Parameters for Polynomial Regression:\", best_params_poly_reg)\n",
    "print(\"Polynomial Regression Metrics:\")\n",
    "print(\"MAE:\", mae_poly_reg)\n",
    "print(\"MSE:\", mse_poly_reg)\n",
    "print(\"MAPE:\", mape_poly_reg)\n",
    "print(\"R-squared:\", r2_poly_reg)\n",
    "print(\"Explained Variance Score:\", evs_poly_reg)\n",
    "print(\"Median Absolute Error:\", medae_poly_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4399586,
     "sourceId": 7553914,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
